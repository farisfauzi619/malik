# -*- coding: utf-8 -*-
"""AAS_DataMining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15WgmsusgSPY8DK9SXBPRNxMthGInU_3I
"""

import pandas as pd
from tqdm import tqdm

# Load the CSV file into a DataFrame
file_path = '/stats.csv'
df = pd.read_csv(file_path, encoding='ascii')

# Show the head of the DataFrame
df_head = df.head()
print(df_head)

import matplotlib.pyplot as plt
import seaborn as sns

# Set the aesthetic style of the plots
sns.set_style('whitegrid')

# Plotting the distribution of 'nilai' column
plt.figure(figsize=(10, 6))
sns.histplot(df['rating'], bins=30, kde=True)
plt.title('Distribution of Values (rating)')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.xscale('log') # Using a log scale due to wide range of values
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Set the aesthetic style of the plots
sns.set_style('whitegrid')

# Since the 'nilai' column contains very large numbers, we'll create a new column for better visualization
# Convert 'nilai' to billions for easier interpretation
# 1 billion = 1e9

# Create a new column 'nilai_in_billions'
df['rating_in_billions'] = df['rating'] / 1e9

# Plotting the distribution of 'nilai_in_billions' column
plt.figure(figsize=(10, 6))
sns.histplot(df['rating_in_billions'], bins=30, kde=True)
plt.title('Distribution of Values (rating) in Billions')
plt.xlabel('Value (in billions)')
plt.ylabel('Frequency')
plt.show()

df_describe = df.describe()
print(df_describe)

# Grouping the data by 'kill' and 'death' to get the sum of 'rating_in_billions'
grouped_data = df.groupby(['kill', 'death'])['rating_in_billions'].sum().reset_index()

# Sorting the grouped data by 'rating_in_billions' to find the highest values
sorted_grouped_data = grouped_data.sort_values(by='rating_in_billions', ascending=False)

# Display the top 5 categories with the highest sum of 'rating_in_billions'
top_categories = sorted_grouped_data.head(5)
print(top_categories)

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
from sklearn.metrics import silhouette_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

xls = pd.ExcelFile('/content/stats.xlsx')
df = xls.parse('stats')

# Preprocessing
# Mengisi data yang kosong
imputer = SimpleImputer(strategy='mean')
df_numeric = df.select_dtypes(include=[np.number])
df_numeric_imputed = pd.DataFrame(imputer.fit_transform(df_numeric), columns=df_numeric.columns)

scaler = StandardScaler()
df_normalized = scaler.fit_transform(df_numeric_imputed)

inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(df_normalized)
    inertia.append(kmeans.inertia_)

# Plot the elbow graph
plt.figure(figsize=(10,6))
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method For Optimal k')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

optimal_clusters = 3
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
kmeans.fit(df_normalized)


df['cluster'] = kmeans.labels_

# menggunakan silhouette score
silhouette_avg = silhouette_score(df_normalized, kmeans.labels_)
print('Silhouette Score:', silhouette_avg)

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_normalized)

# Create a scatter plot
plt.figure(figsize=(10,6))
plt.scatter(df_pca[:, 0], df_pca[:, 1], c=kmeans.labels_, cmap='viridis')
plt.title('Cluster Visualization')
plt.xlabel('PCA Feature 1')
plt.ylabel('PCA Feature 2')
plt.colorbar()
plt.show()

"""Score : 0.14339330536776568"""

# Add the PCA components to the dataframe for better interpretability
clustered_df = pd.DataFrame(df_pca, columns=['PCA1', 'PCA2'])
clustered_df['cluster'] = kmeans.labels_

# Calculate the mean of each PCA component per cluster
cluster_centers = clustered_df.groupby('cluster').mean()

# Display the cluster centers
print(cluster_centers)

# Count the number of instances in each cluster
cluster_counts = clustered_df['cluster'].value_counts()
print(cluster_counts)

# Merge the cluster centers with the original dataframe to see the mean values of the original features per cluster
original_features = df_numeric_imputed.columns.tolist()
cluster_centers_original = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=original_features)
print(cluster_centers_original)

# Import necessary libraries for visualization
import seaborn as sns

# Add the 'kill' feature back to the clustered dataframe
clustered_df['kill'] = df_numeric_imputed['kill']

# Create boxplots to visualize the distribution of 'kill' within each cluster
plt.figure(figsize=(12, 6))
sns.boxplot(x='cluster', y='kill', data=clustered_df)
plt.title('Distribution of kill within Clusters')
plt.xlabel('Cluster')
plt.ylabel('kill')
plt.show()

"""Model ke 2"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from tqdm import tqdm

# Assuming df_normalized is the normalized dataframe used previously
# Determine the range of clusters to evaluate
range_n_clusters = list(range(2, 11))

# Dictionary to store the average silhouette scores for each number of clusters
silhouette_avg_scores = {}

# Use tqdm to show progress
for n_clusters in tqdm(range_n_clusters):
    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 42 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = clusterer.fit_predict(df_normalized)

    # The silhouette_score gives the average value for all the samples.
    silhouette_avg = silhouette_score(df_normalized, cluster_labels)
    silhouette_avg_scores[n_clusters] = silhouette_avg

# Find the optimal number of clusters based on the highest silhouette score
optimal_n_clusters = max(silhouette_avg_scores, key=silhouette_avg_scores.get)
print('Optimal number of clusters:', optimal_n_clusters)
print('Silhouette scores for each cluster count:', silhouette_avg_scores)

# Train a new k-means model with the optimal number of clusters
kmeans_new = KMeans(n_clusters=optimal_n_clusters, random_state=42)
kmeans_new.fit(df_normalized)

# Calculate the silhouette score for the new model
silhouette_avg_new = silhouette_score(df_normalized, kmeans_new.labels_)
print('Silhouette Score for the new k-means model:', silhouette_avg_new)

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Reduce the dimensionality of the data to 2D for visualization
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_normalized)

# Scatter plot of the two clusters
plt.figure(figsize=(8, 8))
for i in range(optimal_n_clusters):
    plt.scatter(df_pca[kmeans_new.labels_ == i, 0], df_pca[kmeans_new.labels_ == i, 1], label='Cluster ' + str(i))
plt.title('2D PCA plot of K-Means Clusters')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend()
plt.show()

